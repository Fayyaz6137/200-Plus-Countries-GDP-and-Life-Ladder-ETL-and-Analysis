# ğŸŒ GDP & Life Ladder ETL Pipeline

An end-to-end ETL (Extract, Transform, Load) pipeline that integrates GDP data and Life Ladder data into a structured SQLite database.

---

## ğŸ“Œ Project Overview

This project implements a modular ETL architecture using Python.

The pipeline:

1. Extracts GDP data from a CSV file
2. Extracts Life Ladder data from an API
3. Transforms and cleans both datasets
4. Integrates the datasets
5. Loads the final dataset into a SQLite database

---
## Visualizations
<img width="493" height="385" alt="Picture1" src="https://github.com/user-attachments/assets/0e9979a5-8802-4657-a2cd-09e67e02e627" />

The project includes:
- GDP vs Life Ladder regression plot
- Historical trend comparison
- Country-level happiness distribution


## See Power BI Dashboard
ğŸ“Œ Interactive dashboard available here:  
ğŸ‘‰ **[Dashboard Link](https://app.powerbi.com/view?r=eyJrIjoiNmFkN2MyNmUtYmQyYi00ZTRhLTg4NTQtODlmMzM1YTM1NmUzIiwidCI6IjMyYjI3ZjU0LTA5ZmItNDhhZi05YzE3LTBmOThhNWQ1OThiZiIsImMiOjh9)**

---

## ğŸ—ï¸ Architecture
```bash
Extract â†’ Transform â†’ Integrate â†’ Load â†’ SQLite
```
---


## Data Dictionary
| Column Name           | Data Type | Description                                                                                                                                                   | Example       |
| --------------------- | --------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------| ------------- |
| `Country name`        | String    | Name of the country for which the data is recorded.                                                                                                           | `Italia`      |
| `year`                | Integer   | Year in which the data was observed.                                                                                                                          | `2025`        |
| `Life Ladder`         | Float     | Numeric indicator representing a country-level Happiness Score.                                                                                               | `2.375`       |
| `Log GDP per capita`  | Float     | Numeric indicator representing a Logged Value of GDP Per Capita.                                                                                              | `7.697`       |

---

## ğŸ“‚ Project Structure
```bash
gdp-life-ladder-etl/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ notebooks/
â”‚      â”œâ”€â”€ analysis Jupytor file
â”œâ”€â”€ data/
â”‚      â”œâ”€â”€ raw/ csv file
â”‚      â”œâ”€â”€ processed/ sqllite3 db file
â””â”€â”€ src/
â”‚      â”œâ”€â”€ extractors/ 
â”‚      â”‚      â”œâ”€â”€ api_reader.py
â”‚      â”‚      â”œâ”€â”€ csv_reader.py
â”‚      â”œâ”€â”€ transformers/ 
â”‚      â”‚      â”œâ”€â”€ api_data_transformer.py
â”‚      â”‚      â”œâ”€â”€ csv_data_transformer.py
â”‚      â”‚      â”œâ”€â”€ integration.py
â”‚      â”œâ”€â”€ loaders/
â”‚      â”‚      â”œâ”€â”€ sqlite3_loader.py
â”‚      â””â”€â”€ utils/
â”‚            â”œâ”€â”€ config.py
â”œâ”€â”€ README.md.txt
â”œâ”€â”€ PowerBI Dashboard Link.txt
â””â”€â”€ requirements.txt
```

---

## ğŸ› ï¸ Technologies Used

- Python 3
- Pandas
- Requests
- SQLite3
- Docker
- Docker Compose

---

## ğŸš€ How to Run

### Run Locally

```bash
pip install -r requirements.txt

python main.py
```
---

## ğŸ³ Run with Docker
```bash
docker compose up --build
```
The SQLite database file will be created inside the data/processed folder.

---

## ğŸ” ETL Steps
1ï¸âƒ£ Extract

* CSV Reader
* API Reader

2ï¸âƒ£ Transform

* Data Cleaning
* Type Casting
* Filtering common countries

3ï¸âƒ£ Integration

Merging datasets on country name

4ï¸âƒ£ Load

Save final dataset into SQLite database

---

## ğŸ“Š Output

* Integrated dataset stored in SQLite database
* Cleaned and merged GDP + Life Ladder data

---

## ğŸ¯ Key Learning Outcomes

* Modular ETL design
* Data integration from multiple sources
* SQLite database loading
* Dockerizing data pipelines
* Clean project structuring

---

## ğŸ“Œ Future Improvements

* Add logging instead of print statements
* Add exception handling
* Add unit tests
* Add scheduling (Airflow / Cron)
* Containerize with PostgreSQL instead of SQLite